{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install extra packages: uncomment if in google collab\n",
    "\n",
    "#holoviews\n",
    "#hvplot\n",
    "#toolz\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recitation 2: Logic gates and information theory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goal of this tutorial is to get you geared up for solving the homework in an efficient manner using python. We'll learn how to compute mutual information between two discrete random variables using numpy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information theory is the mathematical theory that aims to describes how to communicate efficiently (with low error and at a fast rate) through an imperfect channel. As we've seen in class, this is a major tool for analyzing biological systems, since it provides a way to quantify the non-linear coupling between random variables. During this tutorial, we will cover some of the concepts central to information theory and how to compute them using python. \n",
    "\n",
    "There is a very interesting history about how information theory came about, with big contributions from the [\"Young Turks\"](https://en.wikipedia.org/wiki/Young_Turks_(Bell_Labs)), a group of scientists and mathematicians working at Bell Labs. Of utmost importance, Claude Shannon, developed the foundations in a landmark paper titled [A Mathematical Theory of Communication](http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf). I highly recommend for you to read it, as it is a beautiful display of probabilistic and mathematical thinking. \n",
    "\n",
    "In this paper, Shannon defined the surprise (or information gain) of learning that an event $x$ with probability $p(x)$ will occur as:\n",
    "\n",
    "$$\n",
    "\\text{surprise / self-information (x)} = \\mathrm{log}_2 \\left( \\frac{1}{p(x)} \\right)\n",
    "$$\n",
    "\n",
    "This quantity is measured in a unit called bits. Low-probability events will have high surprise, while an event that has probability 1 of occurring has zero surprise. The $\\mathrm{log}$ is there so that if we observe multiple events, the total surprise is additive. The $\\mathrm{log}$ is abse 2 so that if we learn that an event with probability $\\frac{1}{2}$ happened, the surprise is 1 (we'll calculate this later on), we gain 1 bit of information. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy is the average surprise of learning a random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in class, for a discrete r.v. $X$ that takes values $x_1, x_2, ...,x_n$ with probabilities $p_1, p_2, ..., p_n$ with $\\sum_{i} P(X = x_i) = \\sum_i^n p_i = 1$, the entropy of X is defined as the expected or average surprise of learning the value of X: \n",
    "\n",
    "$$\n",
    "H(X) = \\mathbb{E}_X \\left[ \\mathrm{log_2} \\, \\left( \\frac{1}{P(X)} \\right) \\right] \\\\[1.4em]\n",
    "= \\sum_{i = 1}^n p_i \\, \\mathrm{log_2}\\, \\left( \\frac{1}{p_i} \\right)\\\\[1.2em]\n",
    "= - \\sum_{i=1}^n  p_i \\, \\mathrm{log_2} \\, \\left( p_i \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the second equality arises from evaluating the formula of the generalized expectation or [law of unconscious statistician (LOTUS)](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician). **Note that the entropy doesn't depend on the values the random variable takes, but only on the probability values !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the entropy for a binary random variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:22:07.364167Z",
     "start_time": "2020-10-06T23:22:06.986554Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import holoviews as hv\n",
    "import string\n",
    "import seaborn as sns \n",
    "\n",
    "sns.set_style('dark')\n",
    "hv.extension('bokeh')\n",
    "\n",
    "#%load_ext blackcellmagic\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: write down a function that returns the entropy of a Bernoulli random variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:26:21.283032Z",
     "start_time": "2020-10-06T23:26:21.277495Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_entropy(p):\n",
    "    \"Returns the entropy of a Bernoulli r.v. with probability p\"\n",
    "    \n",
    "    #Write your code here\n",
    "    entropy = \n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T00:20:26.244011Z",
     "start_time": "2020-10-07T00:20:26.238636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the function ! \n",
    "binary_entropy(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our function works. Now let's see if it actually returns the right values by calculating the entropy by hand. \n",
    "\n",
    "The entropy of a Bernoulli random variable with probability of success $p = 0.5$ is  : \n",
    "\n",
    "$$\n",
    "H(X) = \\mathbb{E}_X (- \\mathrm{log} \\, P(X)) \\\\[1.4em]\n",
    "= \\sum_{x \\in \\{0,1 \\}} P(X=x) \\,  \\mathrm{log}_2 \\, \\left(\\frac{1}{P(X = x)} \\right)\\\\[1em]\n",
    "= P(X = 0) \\times \\mathrm{log_2} \\frac{1}{P(X = 0)} + P(X = 1) \\times \\mathrm{log_2} \\frac{1}{P(X = 1)}\\\\[1em]\n",
    "= \\frac{1}{2} \\times \\mathrm{log_2} (2) + \\frac{1}{2} \\times \\mathrm{log_2} (2)\\\\[1em]\n",
    "\\mathrm{log_2}(2) = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we do get the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:26:25.438776Z",
     "start_time": "2020-10-06T23:26:25.433937Z"
    }
   },
   "outputs": [],
   "source": [
    "# Should return 1\n",
    "binary_entropy(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a set of values for different probabilities $p_i$, and plot the resulting curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:25:47.543758Z",
     "start_time": "2020-10-06T23:25:47.541451Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define an array of probability of success of \n",
    "# Bernoulli r.v.s\n",
    "ps = np.linspace(0, 1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:29:02.829296Z",
     "start_time": "2020-10-06T23:29:02.825345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initalize list \n",
    "entropies = []\n",
    "\n",
    "for p in ps: \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, we got an error! Let's inspect the binary entropy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look \n",
    "binary_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we got a Not-a-Number (NaN) value when we evaluate `binary_entropy()` with `p` = 0 and `p` = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:32:02.913111Z",
     "start_time": "2020-10-06T23:32:02.906776Z"
    }
   },
   "outputs": [],
   "source": [
    "#We need to avoid this! \n",
    "np.log2(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Write a function that returns the entropy for a random variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:38:12.718973Z",
     "start_time": "2020-10-06T23:38:12.714525Z"
    }
   },
   "outputs": [],
   "source": [
    "ps = np.array([0.5, 0.5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:38:24.848131Z",
     "start_time": "2020-10-06T23:38:24.841478Z"
    }
   },
   "outputs": [],
   "source": [
    "ps[np.nonzero(ps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:39:34.105251Z",
     "start_time": "2020-10-06T23:39:34.098525Z"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(ps): \n",
    "    \n",
    "    # Get nonzero indices\n",
    "    nz = \n",
    "\n",
    "    # Compute entropy for nonzero indices\n",
    "    entropy = \n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:39:37.646333Z",
     "start_time": "2020-10-06T23:39:37.640604Z"
    }
   },
   "outputs": [],
   "source": [
    "entropy(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an array of probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:41:25.494634Z",
     "start_time": "2020-10-06T23:41:25.488999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Intialize empty array\n",
    "p_ = #\n",
    "\n",
    "q_ = #\n",
    "\n",
    "ps = np.hstack([p_.reshape(-1,1), q_.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-evaluate the entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:42:36.752326Z",
     "start_time": "2020-10-06T23:42:36.748072Z"
    }
   },
   "outputs": [],
   "source": [
    "entropies = []\n",
    "\n",
    "for i in range(len(ps)): \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see that we got no error back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:42:47.148280Z",
     "start_time": "2020-10-06T23:42:47.142103Z"
    }
   },
   "outputs": [],
   "source": [
    "entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the entropies ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T00:22:06.635406Z",
     "start_time": "2020-10-07T00:22:06.578844Z"
    }
   },
   "outputs": [],
   "source": [
    "entropy_plot = hv.Curve((p_success, entropies)).opts(\n",
    "    title=\"Bern(p) entropy\", xlabel=\"probability of success\", ylabel=\"entropy\", color = 'salmon', \n",
    "    padding = 0.1\n",
    ")\n",
    "\n",
    "entropy_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this plot ring a bell ? In lecture we saw that the variance of a Bernoulli random variable is $p(1-p)$. Let's quickly make a function of this formula and plot the variances for the $p_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:44:49.610347Z",
     "start_time": "2020-10-06T23:44:49.606291Z"
    }
   },
   "outputs": [],
   "source": [
    "def variance_bernoulli(p): \n",
    "    return p*(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:45:14.293682Z",
     "start_time": "2020-10-06T23:45:14.289640Z"
    }
   },
   "outputs": [],
   "source": [
    "variances = [  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:58:31.839186Z",
     "start_time": "2020-10-06T23:58:31.828806Z"
    }
   },
   "outputs": [],
   "source": [
    "variance_plot = hv.Curve((p_success, variances)).opts(\n",
    "    title=\"Bern(p) variance\", xlabel=\"probability of success\", ylabel=\"entropy\", color = 'dodgerblue', \n",
    "    padding = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T23:58:35.515673Z",
     "start_time": "2020-10-06T23:58:35.472389Z"
    }
   },
   "outputs": [],
   "source": [
    "variance_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T00:23:46.879088Z",
     "start_time": "2020-10-07T00:23:46.765602Z"
    }
   },
   "outputs": [],
   "source": [
    "variance_plot + entropy_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying information from coupled random variables \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the goal of this tutorial is to get you ready for computing the mutual information of noisy logic gates. In order to do so we have define a couple more concepts in order to make this tutorial self-contained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Kullback - Leibler divergence quantifies the *distance* between distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building up towards the mutual information, an important metric to fully grasp the concepts is the Kullback-Leibler (KL) divergence. It is a metric that quantifies the distance between two probability distributions. Let $\\mathbf{q} = (q_1, q_2, ..., q_n)$ and $\\mathbf{p} = (p_1, p_2, .., p_n)$ be two discrete PMFs. The KL divergence between $\\mathbf{q}$ and $\\mathbf{p}$ is defined as: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\text{D}( \\mathbf{q} || \\mathbf{p} ) = \\sum_j q_j \\mathrm{log} \\left( \\frac{1}{p_j} \\right)- \\sum_j q_j \\mathrm{log} \\left(\\frac{1}{q_j} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric thus represents the difference between the average surprise we will experience when the actual probabilities are $\\mathbf{q}$ but we are instead using $\\mathbf{p}$ as our current guess for $\\mathbf{q}$  and the average surprise of  $\\mathbf{q}$. \n",
    "\n",
    "The intuition is that because, we're using $\\mathbf{p}$ as a proxy for $\\mathbf{q}$, the term on the left $\\mathbb{E}_q [ \\mathrm{log_2} (1 / p)]$ will always be higher than the entropy of the true variable $\\mathbb{E}_q [ \\mathrm{log_2} (1 / q)]$. In other words, we will always be at least as surprised or more when working with an estimate of the true distribution that with the true distribution itself. There is a neat way to prove that the KL divergence is nonnegative using one of the most important inequalities in stats: the Jensen Inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief detour: Jensen's inequality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be a random variable. If $g$ is a convex function (in the sense that $g''(x) > 0$), then \n",
    "\n",
    "$$\n",
    "\\mathbb{E}(g(X)) > g ( \\mathbb{E} (X))\n",
    "$$\n",
    "\n",
    "if $g$ is a concave function, then the equality flips. Here is are some intuitive examples of Jensen's inequality. \n",
    "\n",
    "$$\n",
    "\\mathrm{Var} (x) = \\mathbb{E} (x^2) - \\mathbb{E} (x)^2. \\\\[.8em]\n",
    "\\text{because } \\mathrm{Var}(x) \\ge 0 \\\\[.8em]\n",
    "\\mathbb{E}(x^2) \\ge (\\mathbb{E}(x))^2 \\\\[.8em]\n",
    "\\text{reciprocally because log is a concave function}  \\\\[.7em]\n",
    "\\mathbb{E}(\\mathrm {log}(x)) \\le  \\mathrm \\log \\left(\\mathbb{E} (X) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like conditional probabilities, we can have conditional entropies. The conditional entropy $H(X|Y)$ is the information you gain from observing a random variable $X$ conditioning on knowing the values of another random variable $Y$. Another way to think of the conditional entropy is as the average surprise of the conditional distribution $P(X | Y)$ after observing the distribution $P(Y)$.\n",
    "\n",
    "The conditional entropy is defined as : \n",
    "\n",
    "$$\n",
    "H(X | Y) = \\mathbb{E}_{P(y) } \\left[ - \\mathrm{log_2} P(X | Y) \\right] = - \\sum_{y \\in Y} P(Y = y) \\, \\mathrm{log}_2 \\, P(X | Y = y)  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the conditional entropy $H(Y|X)$ can be written as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(Y|X)  = \\mathbb{E}_{P(X)} \\left[ - \\mathrm{log_2} \\, P(Y|X) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using both of this notions, we can finally now define the mutual information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutual information of two discrete random variables $X$, $Y$ ($\\text{I(X,Y)}$) is the reduction in uncertainty of a random variable $X$ when get to know see the conditional distribution with another r.v. $(X|Y)$. Congruent with this notion, the mutual information is defined as:\n",
    "\n",
    "$$\n",
    "\\text{I(X,Y)} = \\text{H} (X) - \\text{H}(X|Y) = \\text{H} (Y) - \\text{H} (Y | X)\n",
    "$$\n",
    "\n",
    "Where $\\text{H}(X)$ is the entropy of $X$. A common way to visualize the concepts of entropy, conditional entropy and mutual information is the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T00:54:09.925223Z",
     "start_time": "2020-10-07T00:54:09.922455Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T00:57:42.021388Z",
     "start_time": "2020-10-07T00:57:42.015401Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(\n",
    "    url = 'https://upload.wikimedia.org/wikipedia/commons/2/2e/Conditional_entropy.png',\n",
    "    format = 'png',\n",
    "    width = 500,\n",
    "    height = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mutual information is the KL divergence between the joint and the marginal distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that by definition, the mutual information between two discrete random variables is the KL divergence between the joint and the marginal distributions, i.e.: \n",
    "$$\n",
    "\\text{MI(X,Y)} = \\sum_{y \\in Y} \\sum_{x \\in X} P_{X,Y}(x, y) \\,  \\mathrm{log_2} \\left( \\frac{P_{X,Y}(x,y)}{P_{X}(x)P_Y(y))} \\right)\n",
    "$$\n",
    "\n",
    "This is a nice conceptual version, because we can see that in the limit where the joint and the marginal distributions are equal, i.e., the r.v.s X and Y are independent, the mutual information is close to zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying mutual information on the english language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T03:04:50.878045Z",
     "start_time": "2020-10-07T03:04:50.831161Z"
    }
   },
   "outputs": [],
   "source": [
    "import toolz as tz\n",
    "from toolz import curried as c\n",
    "from collections import Counter\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:57:37.386558Z",
     "start_time": "2020-10-08T16:57:37.383341Z"
    }
   },
   "outputs": [],
   "source": [
    "#path = '../../bi195_tutorials/data/california_love.txt'\n",
    "#! wget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:57:37.947387Z",
     "start_time": "2020-10-08T16:57:37.814567Z"
    }
   },
   "outputs": [],
   "source": [
    "! head -10 {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are empty lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:57:39.141213Z",
     "start_time": "2020-10-08T16:57:39.137883Z"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:23:45.939097Z",
     "start_time": "2020-10-08T16:23:45.936378Z"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:23:46.141624Z",
     "start_time": "2020-10-08T16:23:46.137805Z"
    }
   },
   "outputs": [],
   "source": [
    "re.sub(pattern = '[^a-zA-Z]+', repl = '', string = 'Hello :')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This uses on-memory filtering, so this function alone makes three passes to the data. Caution is neeeded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:27:04.564360Z",
     "start_time": "2020-10-08T16:27:04.558504Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(path_to_file): \n",
    "    \"\"\"\n",
    "    Return a list of strings from a text file.\n",
    "    \"\"\"\n",
    "    with open(path_to_file, 'r') as f: \n",
    "        corpus = f.readlines()\n",
    "\n",
    "    # Remove empty lines \n",
    "    corpus = list(filter(not_empty_line, corpus))\n",
    "    \n",
    "    # Substitute all symbols not in the alphabet \n",
    "    # and lowercase all words \n",
    "    corpus = [re.sub('[^A-Za-z]', ' ', line.strip().lower()) for line in corpus]\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:57:44.917461Z",
     "start_time": "2020-10-08T16:57:44.913518Z"
    }
   },
   "outputs": [],
   "source": [
    "lines = read_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:57:45.240113Z",
     "start_time": "2020-10-08T16:57:45.237693Z"
    }
   },
   "outputs": [],
   "source": [
    "all_lines = ' '.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:57:45.861421Z",
     "start_time": "2020-10-08T16:57:45.857321Z"
    }
   },
   "outputs": [],
   "source": [
    "all_lines[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:27:05.512231Z",
     "start_time": "2020-10-08T16:27:05.507390Z"
    }
   },
   "outputs": [],
   "source": [
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:27:07.045363Z",
     "start_time": "2020-10-08T16:27:07.040118Z"
    }
   },
   "outputs": [],
   "source": [
    "alphabet= set(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:27:08.947947Z",
     "start_time": "2020-10-08T16:27:08.944341Z"
    }
   },
   "outputs": [],
   "source": [
    "alphabet_letters = sorted(list(alphabet))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:27:09.144021Z",
     "start_time": "2020-10-08T16:27:09.140269Z"
    }
   },
   "outputs": [],
   "source": [
    "alphabet_letters[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:27:10.495125Z",
     "start_time": "2020-10-08T16:27:10.489781Z"
    }
   },
   "outputs": [],
   "source": [
    "len(alphabet_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T16:27:30.295885Z",
     "start_time": "2020-10-08T16:27:30.291527Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text(text): \n",
    "    \"Substitutes symbols in a string with a space.\"\n",
    "    text = re.sub('[^A-Za-z]', ' ', text.strip().lower())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T15:58:13.963650Z",
     "start_time": "2020-10-08T15:58:13.959299Z"
    }
   },
   "outputs": [],
   "source": [
    "def stream_file(fname): \n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T15:59:40.674856Z",
     "start_time": "2020-10-08T15:59:40.668721Z"
    }
   },
   "outputs": [],
   "source": [
    "next(stream_file(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T15:59:42.457287Z",
     "start_time": "2020-10-08T15:59:42.453572Z"
    }
   },
   "outputs": [],
   "source": [
    "g = stream_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T15:59:48.815285Z",
     "start_time": "2020-10-08T15:59:48.809714Z"
    }
   },
   "outputs": [],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a function that allows us to stream large files and get kmer counts already, so we can just go ahead and apply it to large datasets (see exercise at the final of the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:33.233872Z",
     "start_time": "2020-10-08T17:04:33.229328Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_counts_kmer_streaming(path_to_file, k=1): \n",
    "    \"\"\"\n",
    "    Returns a dictionary of kmer counts using streaming.\n",
    "    This function is designed to handle large files on-disk.\n",
    "    \n",
    "    Params \n",
    "    ------\n",
    "    path_to_file(str):\n",
    "        Path to the file to process. \n",
    "    \n",
    "    k(int, default = 1):\n",
    "        Size of the k-mer. \n",
    "        \n",
    "    Returns \n",
    "    -------\n",
    "    dict:\n",
    "        A dictionary whose keys are unique k-mers found in the file \n",
    "        and values are the frequencies of appearance. \n",
    "    \"\"\"\n",
    "    \n",
    "    return tz.pipe(\n",
    "        path_to_file,\n",
    "        stream_file, # Initializes line generator \n",
    "        c.filter(not_empty_line), # Gets lines with strings (not symbols or whitespaces)\n",
    "        c.map(process_text), # Eliminate symbols \n",
    "        c.map(str.strip), # Eliminate white space at the end of line\n",
    "        c.map(str.split), # Split into words\n",
    "        tz.concat, # Concatenate word iterator\n",
    "        c.map(c.sliding_window(k)), # Make k-mer tuples\n",
    "        tz.concat, # Concatenate tuple iterator\n",
    "        c.map(''.join), # Unpack tuples \n",
    "        tz.frequencies # Get kmer frequencies\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:33.514646Z",
     "start_time": "2020-10-08T17:04:33.508454Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = get_counts_kmer_streaming(path, k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:33.893609Z",
     "start_time": "2020-10-08T17:04:33.890808Z"
    }
   },
   "outputs": [],
   "source": [
    "alphabet_letters = sorted(list(set(''.join(list(counts.keys())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:34.755072Z",
     "start_time": "2020-10-08T17:04:34.752088Z"
    }
   },
   "outputs": [],
   "source": [
    "n_letters = len(alphabet_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:35.335661Z",
     "start_time": "2020-10-08T17:04:35.333317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get total counts\n",
    "total_counts = sum(counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:35.799492Z",
     "start_time": "2020-10-08T17:04:35.795856Z"
    }
   },
   "outputs": [],
   "source": [
    "total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:36.570186Z",
     "start_time": "2020-10-08T17:04:36.566218Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sort freqs by alphabetical order \n",
    "freqs = sorted(counts.items(), key = lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:37.103801Z",
     "start_time": "2020-10-08T17:04:37.101417Z"
    }
   },
   "outputs": [],
   "source": [
    "#counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:37.296749Z",
     "start_time": "2020-10-08T17:04:37.292689Z"
    }
   },
   "outputs": [],
   "source": [
    "freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:38.947559Z",
     "start_time": "2020-10-08T17:04:38.943183Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alphabet_dict = dict(zip(alphabet_letters, np.arange(n_letters)))\n",
    "\n",
    "alphabet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:39.443294Z",
     "start_time": "2020-10-08T17:04:39.440560Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_pairs = list(it.product(alphabet_letters, alphabet_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:39.779624Z",
     "start_time": "2020-10-08T17:04:39.776631Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:40.146947Z",
     "start_time": "2020-10-08T17:04:40.143371Z"
    }
   },
   "outputs": [],
   "source": [
    "position_dict = {\n",
    "    (str(i + j)): (alphabet_dict[i], alphabet_dict[j]) for (i,j) in bigram_pairs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:40.280078Z",
     "start_time": "2020-10-08T17:04:40.276436Z"
    }
   },
   "outputs": [],
   "source": [
    "position_dict[('aa')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:40.474439Z",
     "start_time": "2020-10-08T17:04:40.470017Z"
    }
   },
   "outputs": [],
   "source": [
    "position_dict[('ie')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:40.689696Z",
     "start_time": "2020-10-08T17:04:40.686172Z"
    }
   },
   "outputs": [],
   "source": [
    "freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:40.922196Z",
     "start_time": "2020-10-08T17:04:40.919319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize joint distribution matrix \n",
    "joint_distro = np.zeros((n_letters,n_letters))\n",
    "\n",
    "for bigram, counts in freqs: \n",
    "    \n",
    "    # Get indices for given bigram\n",
    "    index = position_dict[bigram]\n",
    "    \n",
    "    # Set counts in array \n",
    "    joint_distro[index] = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:42.350231Z",
     "start_time": "2020-10-08T17:04:42.347172Z"
    }
   },
   "outputs": [],
   "source": [
    "joint_distro = joint_distro / total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:42.665314Z",
     "start_time": "2020-10-08T17:04:42.661594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Double check normalization condition \n",
    "joint_distro.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:04:57.538835Z",
     "start_time": "2020-10-08T17:04:57.533755Z"
    }
   },
   "outputs": [],
   "source": [
    "len(alphabet_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:09:04.331070Z",
     "start_time": "2020-10-08T17:09:04.327827Z"
    }
   },
   "outputs": [],
   "source": [
    "df_joint= pd.DataFrame(\n",
    "    joint_distro, \n",
    "    index = pd.Index(list(alphabet_letters), name = 'first letter'),\n",
    "    columns = pd.Index(list(alphabet_letters), name = 'second letter')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:09:04.638713Z",
     "start_time": "2020-10-08T17:09:04.614603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look \n",
    "df_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop columns and rows with nonzero values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:09:05.917834Z",
     "start_time": "2020-10-08T17:09:05.909884Z"
    }
   },
   "outputs": [],
   "source": [
    "(df_joint != 0 ).any(axis  = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:09:06.820971Z",
     "start_time": "2020-10-08T17:09:06.813574Z"
    }
   },
   "outputs": [],
   "source": [
    "(df_joint != 0 ).any(axis  = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:09:16.046864Z",
     "start_time": "2020-10-08T17:09:16.043096Z"
    }
   },
   "outputs": [],
   "source": [
    "col_selector = (df_joint != 0).any(axis  = 0)\n",
    "row_selector = (df_joint != 0).any(axis  = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:09:16.384687Z",
     "start_time": "2020-10-08T17:09:16.379684Z"
    }
   },
   "outputs": [],
   "source": [
    "row_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:09:27.339982Z",
     "start_time": "2020-10-08T17:09:27.335134Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sel = df_joint.loc[row_selector, col_selector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:11:47.507166Z",
     "start_time": "2020-10-08T17:11:47.500049Z"
    }
   },
   "outputs": [],
   "source": [
    "df_melt = df_sel.unstack().reset_index().rename(columns = {0 : 'P(x,y)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:12:01.703821Z",
     "start_time": "2020-10-08T17:12:01.058856Z"
    }
   },
   "outputs": [],
   "source": [
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:12:31.329147Z",
     "start_time": "2020-10-08T17:12:31.318613Z"
    }
   },
   "outputs": [],
   "source": [
    "df_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt.hvplot.heatmap(\n",
    "    x = 'first letter',\n",
    "    y = 'second letter', \n",
    "    C = 'P(x,y)',\n",
    "    height = 500,\n",
    "    width = 600, \n",
    "    cmap = 'viridis', \n",
    "    title = 'Joint distribution P(x,y)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the joint distribution in place, we can calculate the entropy of the letters in english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:14:45.259434Z",
     "start_time": "2020-10-08T17:14:45.255254Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now let's compute the mutual information between the first and second letters.\n",
    "\n",
    "p_x = df_sel.values.sum(axis = 1)\n",
    "p_y = df_sel.values.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:14:46.899559Z",
     "start_time": "2020-10-08T17:14:46.895717Z"
    }
   },
   "outputs": [],
   "source": [
    "entropy(p_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is indeed what's reported ([see F1 in this post](https://cs.stanford.edu/people/eroberts/courses/soco/projects/1999-00/information-theory/entropy_of_english_9.html) and [Shannon's paper on English language redundancy](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf)).\n",
    "\n",
    "This is super interesting, but **how do we know if this entropy is a lot or a little? In other words, what would be a null model for the entropy of a distribution?**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Proposition : the uniform distribution is the  distribution with maximum entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuititively, we can assert that the above proposition is true because, if a uniform distribution every value is equally likely, and thus whichever value we see sampled, will be \"surprising\". On the other hand, if we think of a very sharp peaked distribution, with the probability mass concentrated about the mean, we would expect that if we sample from the distribution, we would get values close to the mean, and thus, these samples wouldn't be surprising. \n",
    "\n",
    "*Proof:* Let $X$~DUnif(0,n) so that every value $P(x = i) = \\frac{1}{n}$ for $0 \\le i \\le n$ : \n",
    "\n",
    "$$\n",
    "H(X) = \\mathbb{E} \\left[\\mathrm{log_2} \\left( \\frac{1}{p(x)} \\right) \\right] = \\sum_i^n \\frac{1}{n} \\mathrm{log_2} (n) = \\mathrm{log_2 } (n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let $Y$ be an r.v. that takes on values $ \\frac{1}{p_1}, ..., \\frac{1}{p_n} $ with probabilities $p_1, .., p_n$. Then $H(Y) = \\sum_i^n p_i \\mathrm {log} (1 / p_i) = \\mathbb{E}[ \\mathrm{log} Y]$ by evaluating $Y$ and using LOTUS. By Jensen's : \n",
    "\n",
    "$$\n",
    "H(Y) = \\mathbb{E}[ \\mathrm{log} Y]  \\le \\mathrm{log} (\\mathbb{E}(Y)) = \\mathrm{log} (n) = H(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because the entropy of a random variable doesn't depend on its support (i.e. the values the r.v. takes), then $Y$ is unchanges if we change the support from $p_1, .., p_n$ to $a_1, ..., a_n$ . Therefore $X$ which is uniform has entropy at least as large as that of any other r.v. with support on $a_1, a_2, ..., a_n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redundancy is the ratio that one can compress a random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the notion that the uniform distribution is the one distribution with maximum entropy, the information redundancy of a random variable is defined as: \n",
    "\n",
    "$$\n",
    "\\text{redundancy}= 1 - \\frac{H(X) }{H_{ \\text{unif}}} = 1 - \\frac{H(X)}{\\mathrm{log_2} n}\n",
    "$$\n",
    "\n",
    "where $n$ is taken as as the size of the alphabet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the information redundancy of english letters !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:14:51.933198Z",
     "start_time": "2020-10-08T17:14:51.927536Z"
    }
   },
   "outputs": [],
   "source": [
    "entropy(p_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:14:53.170126Z",
     "start_time": "2020-10-08T17:14:53.164619Z"
    }
   },
   "outputs": [],
   "source": [
    "entropy(p_x) / np.log2(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:14:54.837758Z",
     "start_time": "2020-10-08T17:14:54.833986Z"
    }
   },
   "outputs": [],
   "source": [
    "np.log2(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:14:55.307632Z",
     "start_time": "2020-10-08T17:14:55.302654Z"
    }
   },
   "outputs": [],
   "source": [
    "redundancy = 1 - (entropy(p_x)/ np.log2(26))\n",
    "print(f'The redundancy of the english letters is { round(100*redundancy, 2) } %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shannon, indeed a different and very ingenious way of calculating the entropy of the letters in english, by estimating the entropy over words. Let's continue with our calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional distributions have to follow all of the properties of distributions, that is, have nonnegative probabilities and sum to one. We can thus then get the conditional distributions by normalizing the joint distribution over rows and columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:15:19.887138Z",
     "start_time": "2020-10-08T17:15:19.868337Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:16:16.396646Z",
     "start_time": "2020-10-08T17:16:16.393539Z"
    }
   },
   "outputs": [],
   "source": [
    "joint_distro_filt = df_sel.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T17:36:56.009976Z",
     "start_time": "2020-10-08T17:36:56.007230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize over rows\n",
    "p_ygx = (joint_distro_filt / p_x.reshape(-1, 1))\n",
    "\n",
    "# Normalize over columns\n",
    "p_xgy = joint_distro_filt / p_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T19:42:53.003509Z",
     "start_time": "2020-10-08T19:42:52.998574Z"
    }
   },
   "outputs": [],
   "source": [
    "df_xgy = df_sel.div(df_sel.sum(axis=0), axis = 1)\n",
    "\n",
    "df_ygx = df_sel.div(df_sel.sum(axis=1), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T19:43:15.261191Z",
     "start_time": "2020-10-08T19:43:15.255993Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check that both methods match\n",
    "np.all(df_xgy.values ==p_xgy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that conditional distributions sum to 1\n",
    "p_xgy.sum(axis = 0) # over columns\n",
    "p_ygx.sum(axis = 1) # over rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a clever way to compute the conditional entropy from our array directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_wise_information(px):\n",
    "    \"\"\"\n",
    "    Returns a numpy array with element wise information calculated as -log_2(p_i).\n",
    "    This quantity is also know as information or self-information:\n",
    "    https://en.wikipedia.org/wiki/Information_content\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    px (np.array)\n",
    "        Array of individual probabilities, i.e. a probability vector or distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    information (np.array)\n",
    "        Array of element-wise information content.\n",
    "    \"\"\"\n",
    "    if isinstance(px, list):\n",
    "        px = np.array(px)\n",
    "        \n",
    "    # Make a copy of input array\n",
    "    information_content = px.copy()\n",
    "    \n",
    "    # Get indices of nonzero probability values\n",
    "    nz = np.nonzero(information_content)\n",
    "    \n",
    "    # Compute -log_2(p_i) element-wise\n",
    "    information_content[nz] *= - np.log2(information_content[nz])\n",
    "    \n",
    "    return information_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T05:43:45.188979Z",
     "start_time": "2020-10-07T05:43:45.181729Z"
    }
   },
   "outputs": [],
   "source": [
    "h_xgy = np.sum(p_y * element_wise_information(p_xgy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T05:43:58.551788Z",
     "start_time": "2020-10-07T05:43:58.548452Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mutual information, finally!!\n",
    "mi = h_x - h_xgy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T05:44:00.781509Z",
     "start_time": "2020-10-07T05:44:00.776043Z"
    }
   },
   "outputs": [],
   "source": [
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T05:44:26.347856Z",
     "start_time": "2020-10-07T05:44:26.344241Z"
    }
   },
   "outputs": [],
   "source": [
    "h_y = entropy(p_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T05:44:45.104730Z",
     "start_time": "2020-10-07T05:44:45.101359Z"
    }
   },
   "outputs": [],
   "source": [
    "# Joint entropy \n",
    "h_xy = h_x  + h_y - mi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T05:44:47.299410Z",
     "start_time": "2020-10-07T05:44:47.294116Z"
    }
   },
   "outputs": [],
   "source": [
    "h_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:41:06.063467Z",
     "start_time": "2020-10-07T06:41:06.059046Z"
    }
   },
   "outputs": [],
   "source": [
    "redundancy_bigram = 1 - h_xy / np.log2(27**2)\n",
    "\n",
    "print(f'The information redundancy of bigrams is {round(redundancy_bigram*100, 2) } %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief on logic gates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making a binary sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T02:49:07.734912Z",
     "start_time": "2020-10-04T02:49:07.731077Z"
    }
   },
   "outputs": [],
   "source": [
    "x = (0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get all combinations of 0 and 1 by taking the cartesian product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:05:37.868616Z",
     "start_time": "2020-10-04T00:05:37.864277Z"
    }
   },
   "outputs": [],
   "source": [
    "xy = np.array(list(it.product(x,x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:10:20.735614Z",
     "start_time": "2020-10-04T00:10:20.730106Z"
    }
   },
   "outputs": [],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a new vector storing all of the values evaluating the function OR(X,Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T02:50:42.288439Z",
     "start_time": "2020-10-04T02:50:42.283228Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize array \n",
    "or_expression = np.zeros(xy.shape[0])\n",
    "\n",
    "# Evaluate OR gate\n",
    "for ix in range(xy.shape[0]):\n",
    "    # Unpack each row \n",
    "    x, y = xy[ix]\n",
    "    \n",
    "    # Evaluate x OR y\n",
    "    or_expression[ix] = x | y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expressions for AND and XOR are `&` and `^` respectively. You can read about how to implement more logic functions [here](https://www.journaldev.com/42242/logic-gates-in-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:18:51.433504Z",
     "start_time": "2020-10-04T00:18:51.427499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check it out \n",
    "or_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:18:52.193191Z",
     "start_time": "2020-10-04T00:18:52.186682Z"
    }
   },
   "outputs": [],
   "source": [
    "or_expression.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:19:06.931218Z",
     "start_time": "2020-10-04T00:19:06.927027Z"
    }
   },
   "outputs": [],
   "source": [
    "or_truth_table = np.hstack([xy, or_expression.reshape(-1, 1)]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:19:07.631097Z",
     "start_time": "2020-10-04T00:19:07.625105Z"
    }
   },
   "outputs": [],
   "source": [
    "or_truth_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:19:08.996721Z",
     "start_time": "2020-10-04T00:19:08.993977Z"
    }
   },
   "outputs": [],
   "source": [
    "or_df = pd.DataFrame(\n",
    "    or_truth_table, \n",
    "    columns = ['x', 'y', 'z (OR)']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:19:09.478635Z",
     "start_time": "2020-10-04T00:19:09.471084Z"
    }
   },
   "outputs": [],
   "source": [
    "or_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by getting the joint distribution $P(x,z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:25:57.301737Z",
     "start_time": "2020-10-04T00:25:57.297885Z"
    }
   },
   "outputs": [],
   "source": [
    "contingency_table = np.zeros((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:25:58.421801Z",
     "start_time": "2020-10-04T00:25:58.417553Z"
    }
   },
   "outputs": [],
   "source": [
    "x = or_df['x'].values\n",
    "z = or_df['z (OR)'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T02:52:30.027977Z",
     "start_time": "2020-10-04T02:52:30.021561Z"
    }
   },
   "outputs": [],
   "source": [
    "(x == 0) & (z == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T02:52:39.599617Z",
     "start_time": "2020-10-04T02:52:39.594729Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum((x == 1) & (z == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T02:52:52.082563Z",
     "start_time": "2020-10-04T02:52:52.076816Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in [0,1]:\n",
    "    for j in [0,1]: \n",
    "        contingency_table[i,j] = np.sum((x == i) & (z == j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T02:52:57.355260Z",
     "start_time": "2020-10-04T02:52:57.348622Z"
    }
   },
   "outputs": [],
   "source": [
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = np.zeros((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:26:05.956913Z",
     "start_time": "2020-10-04T00:26:05.952650Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,j in zip(x,z):\n",
    "    contingency_table[i,j] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:26:13.480343Z",
     "start_time": "2020-10-04T00:26:13.476221Z"
    }
   },
   "outputs": [],
   "source": [
    "p_xz = contingency_table/ contingency_table.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:26:16.623368Z",
     "start_time": "2020-10-04T00:26:16.617699Z"
    }
   },
   "outputs": [],
   "source": [
    "df_joint = pd.DataFrame(\n",
    "    p_xz, \n",
    "    index = pd.Index(['x = 0', 'x = 1']),\n",
    "    columns = pd.Index(['z = 0', 'z = 1'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T00:26:17.109547Z",
     "start_time": "2020-10-04T00:26:17.099608Z"
    }
   },
   "outputs": [],
   "source": [
    "df_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've uploaded some sequences from PFAM. With the functions above, calculate the redundancy of the aminoacid code for bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Food for thought*: How would you go about getting the redundancy for trigrams ? What data structure would you use if you wanted to use the same approach we did for bigrams ? What other clever tricks you can think of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
